{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.cluster\n",
    "import sklearn.metrics.pairwise\n",
    "import spacy\n",
    "import sentence_transformers\n",
    "\n",
    "#loading the english language small model of spacy, just for stop words\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = list(en.Defaults.stop_words)\n",
    "\n",
    "\n",
    "HEADLINE_CSV = \"../data/mc-onlinenews-mediacloud-20240221094242-content.csv\"\n",
    "\n",
    "# Save outputs to these locations\n",
    "MODEL_PATH = Path(\"../data/models\")\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Headlines from Media Cloud Search Results\n",
    "First, we downloaded data from the [Media Cloud Search tool](https://search.mediacloud.org) to get headlines from the \"Nigeria - National\" collection which matched the following keyword query: \n",
    "```(\"African caravans\" OR \"loss and damage\" OR \"climate justice\" OR \"african activist for climate justice\" OR \"oxfam nigeria\" OR Nasarawa OR cop28 OR cop27) -(gaza)```\n",
    "\n",
    "The output from the Media Cloud Search tool was downloaded directly to a CSV file, which we will read in and analyze. This data could also be programmatically obtained via the Media Cloud API, but it is helpful to have a domain expert come up with query key terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_df = pd.read_csv(HEADLINE_CSV, index_col=False, header=0)\n",
    "headlines_df[\"publish_date\"] = pd.to_datetime(headlines_df[\"publish_date\"]).dt.date\n",
    "print(headlines_df.shape)\n",
    "headlines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check attention article count\n",
    "fig, ax = plt.subplots(figsize=[24,6])\n",
    "sns.countplot(x=headlines_df[\"publish_date\"].sort_values())\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sentence Transformers to Cluster Headlines\n",
    "This section creates vector representations of the headlines using sentence transformers. \n",
    "Each headline is assigned a numerical vector representation, which are then clustered into topics. \n",
    "In order to make sense of the topics, you can view most frequent words in each cluster and the headlines closest to the cluster's center point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast sentence transformer model\n",
    "sent_transformer = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = headlines_df[\"title\"].to_numpy()\n",
    "sent_embeds = sent_transformer.encode(headlines, show_progress_bar=True)\n",
    "normalized_sent_embeds = sklearn.preprocessing.normalize(sent_embeds)\n",
    "print(\"Sanity check (#docs, embedding size):\", normalized_sent_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random state helps keep results reproducible, so others can re-run the notebook\n",
    "# We did see similar clusters when re-running with different random states. \n",
    "kmeans_model = sklearn.cluster.KMeans(15, random_state=199)\n",
    "kmeans_fit = kmeans_model.fit(normalized_sent_embeds)\n",
    "with open(MODEL_PATH / \"kmeans_explore.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(kmeans_fit, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_df['cluster_label'] = kmeans_fit.labels_\n",
    "headlines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count documents in each cluster\n",
    "headlines_df.groupby(\"cluster_label\").count()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic frequency over date\n",
    "# Sanity check attention article count\n",
    "date_cluster_group_df = headlines_df.groupby(by=[\"publish_date\", \"cluster_label\"]).count().reset_index()[[\"publish_date\", \"cluster_label\", \"id\"]]\n",
    "date_cluster_group_df = date_cluster_group_df.sort_values(\"publish_date\")\n",
    "date_cluster_group_df = date_cluster_group_df.rename(columns = {\"id\": \"daily_count\"})\n",
    "print(date_cluster_group_df.head())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[24,6])\n",
    "sns.lineplot(date_cluster_group_df, x=\"publish_date\", y=\"daily_count\", hue=\"cluster_label\", \n",
    "             palette =['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080',  '#fffac8', '#800000', '#aaffc3']).set_title(\"Cluster volume over time\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is shows a few spikes which are interesting for us around the Cop28 event. The spike of cluster 13 at the beginning of December 2023, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pipeline for top words in each cluster\n",
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(stop_words=stopwords)\n",
    "doc_word_counts = count_vectorizer.fit_transform(headlines_df[\"title\"])\n",
    "print(\"Sanity check shape is (# docs, #words):\", doc_word_counts.shape)\n",
    "vocabulary = count_vectorizer.get_feature_names_out()\n",
    "print(\"Sanity check vocabulary:\", vocabulary[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(document_idxs, doc_word_counts, vocabulary, n=20): \n",
    "    \"\"\"Given the indices of selected documents, sum the word counts within only those\n",
    "    selected docs, then return the top n words that are most frequent in the cluster\n",
    "\n",
    "    Args:\n",
    "        document_idxs (1d numpy array): locations of selected documents\n",
    "        doc_word_counts (2d numpy array): (# docs, vocab size), count of each vocab word in the doc\n",
    "        vocabulary (list[str]): vocabulary as a list\n",
    "        n (int): top n most frequent words to return \n",
    "\n",
    "    Returns:\n",
    "        list[str]: the top n most frequent words in the cluster in decreasing order\n",
    "    \"\"\"\n",
    "    selected_docs = doc_word_counts[document_idxs]\n",
    "    word_counts_in_selected_docs = np.resize(np.sum(selected_docs, axis=0), vocabulary.shape)\n",
    "    # Sort word counts in increasing order, so last element is biggest\n",
    "    top_word_idx = np.argsort(word_counts_in_selected_docs)[-n:]\n",
    "    # Reverse order to give most frequent word first\n",
    "    top_words = vocabulary[top_word_idx].tolist()\n",
    "    top_words.reverse()\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_closest_to_centroid(centroid_vector, doc_embeddings, doc_texts, n=20):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        centroid_vector (np.array): vector that's the mathematical center of the cluster in vector space, shape (embedding_size, )\n",
    "        doc_embeddings (np.array): embeddings of the document in the cluster, shape (#docs, embedding_size)\n",
    "        doc_texts (pd.DataFrame): dataframe of documents in the cluster\n",
    "        n (int, optional): Number of documents to return . Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The subset of the dataframe for the top n documents closest to the center of the cluster\n",
    "    \"\"\"\n",
    "    distances = sklearn.metrics.pairwise.cosine_distances(doc_embeddings, centroid_vector.reshape(1, -1))[:,-1]\n",
    "    # Indices of elements closest to center (sort in increasing order)\n",
    "    sorted_indices = np.argsort(distances)[:n]\n",
    "    typical_headlines = doc_texts.loc[sorted_indices]\n",
    "    return typical_headlines\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_article_volume_for_cluster_over_time(dataframe, cluster_id, time_col=\"publish_date\", cluster_col=\"cluster_label\", match_placeholder=\"matches_cluster\"):\n",
    "    \"\"\"Plots the daily volume of articles for that cluster vs all other articles\n",
    "    \"\"\"\n",
    "    sorted_by_date = dataframe.sort_values(time_col)\n",
    "    sorted_by_date[match_placeholder] = (sorted_by_date[cluster_col] == cluster_id)\n",
    "    fig, ax = plt.subplots(figsize=[24,6])\n",
    "    sns.countplot(sorted_by_date, x=time_col, hue=match_placeholder).set(title=f\"Volume for cluster {cluster_id}\")\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option(\"display.max_rows\", 30, \"display.max_columns\", None, 'max_colwidth', 800)\n",
    "centroids = kmeans_fit.cluster_centers_\n",
    "for i in range(15):\n",
    "    print(\"Cluster\", i)\n",
    "    \n",
    "    cluster_element_idx = np.where(headlines_df[\"cluster_label\"] == i)[0]\n",
    "    print(\"\\tNumber of docs in cluster:\", cluster_element_idx.shape)\n",
    "    # Top words can give you a general idea of what's talked about in the topic, but it can be difficult to interpret \n",
    "    top_words = get_top_words(cluster_element_idx, doc_word_counts, vocabulary)\n",
    "    print(\"\\tTop words:\", top_words)\n",
    "\n",
    "    # What are the \"prototypical\" headlines for each cluster? These are the headlines closest to the cluster center for each cluster\n",
    "    center = centroids[i]\n",
    "    cluster_texts = headlines_df.loc[cluster_element_idx].reset_index()\n",
    "    cluster_embeddings = normalized_sent_embeds[cluster_element_idx]\n",
    "    typical_headlines = get_docs_closest_to_centroid(center, cluster_embeddings, cluster_texts)\n",
    "    print(\"\\tTypicial headlines close to center of cluster\")\n",
    "    display(typical_headlines[[\"media_name\", \"publish_date\", \"title\"]])\n",
    "    \n",
    "    \n",
    "    # A sample of random headlines from the cluster gives an indication of whether the cluster's topic is consistent or makes less sense\n",
    "    # further from the centroid\n",
    "    print(\"\\tRandom headlines\")\n",
    "    display(headlines_df.loc[cluster_element_idx].sample(10)[[\"media_name\", \"publish_date\", \"title\"]])\n",
    "\n",
    "    print(\"\\tArticle volume for cluster\")\n",
    "    show_article_volume_for_cluster_over_time(headlines_df, i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to identify cluster 4 as relevant specifically to climate change initiatives and cluster 13 as relevant to Cop28, both of which are interesting to Oxfam's work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which headlines mention Oxfam? \n",
    "Which headlines mention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is 'oxfam' in the vocabulary\n",
    "print(\"Sanity check that 'oxfam' is in the vocabulary:\", \"oxfam\" in vocabulary)\n",
    "print(list(vocabulary).index(\"oxfam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxfam_matches = headlines_df[headlines_df[\"title\"].str.contains(\"oxfam\", flags=re.IGNORECASE)]\n",
    "print(len(oxfam_matches))\n",
    "display(oxfam_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = oxfam_matches[\"cluster_label\"]).set_title(\"How many headlines in different clusters mention 'oxfam'?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
